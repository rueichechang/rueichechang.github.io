<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ruei-Che Chang </title> <meta name="author" content="Ruei-Che Chang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rueichechang.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/rueichechangcv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Ruei-Che</span> Chang </h1> <p class="desc">Human-AI Interaction | Accessibility</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rueiche-480.webp 480w,/assets/img/rueiche-800.webp 800w,/assets/img/rueiche-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/rueiche.png?5713026d785521a667adc5a30fedd34d" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="rueiche.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="mailto:%20rueiche@umich.edu"><i class="fa-solid fa-envelope"></i><u>rueiche@umich.edu</u></a><br> <a href="https://x.com/RueiChe" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-twitter"></i>Twitter</a><br> <a href="https://scholar.google.com.tw/citations?user=G14TzOEAAAAJ&amp;hl=zh-TW" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-google-scholar"></i>Google Scholar</a><br> <a href="../assets/pdf/rueichechangcv.pdf"><i class="fa-solid fa-file-pdf"></i>Curriculum Vitae</a><br> </div> </div> <div class="clearfix"> <p>I am a Human Computer Interaction researcher and PhD Candidate supported by the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025" rel="external nofollow noopener" target="_blank">Ô£ø Apple Scholars in AIML PhD Fellowship</a>, <a href="https://cse.engin.umich.edu/stories/ruei-che-chang-receives-rackham-international-student-fellowship" rel="external nofollow noopener" target="_blank">Rackham International Students Fellowship</a>, and Weinberg Cognitive Science Fellowship.</p> <p>I am a part of <a href="https://guoanhong.com/" rel="external nofollow noopener" target="_blank">Human-AI Lab</a> at <a href="https://cse.engin.umich.edu/" rel="external nofollow noopener" target="_blank">Department of Computer Science, University of Michigan</a>, advised by <a href="https://guoanhong.com/" rel="external nofollow noopener" target="_blank">Anhong Guo</a>.</p> <p>My research focuses on developing agents that can understand and describe real-world surroundings for individuals who are blind or visually impaired, with a technical core that is adaptable to broader contexts.</p> <p>During my PhD study, I interned at <a href="https://about.meta.com/realitylabs/" rel="external nofollow noopener" target="_blank">Meta Reality Labs</a>. Prior to that, I graduated with a Master degree in Computer Science at <a href="https://web.cs.dartmouth.edu/" rel="external nofollow noopener" target="_blank">Dartmouth College</a> and a Bachelor degree in Electrical Engineering at <a href="https://www.ee.ncku.edu.tw/en/" rel="external nofollow noopener" target="_blank">National Cheng Kung University</a> in Taiwan.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Mar 03, 2025</th> <td> honored to receive <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025" rel="external nofollow noopener" target="_blank">Ô£ø Apple Scholars in AIML PhD fellowship (AI for Accessibility)</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 27, 2025</th> <td> excited to share that I will be interning at Adobe research in Seattle this summer! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 08, 2024</th> <td> our work <a href="https://cse.engin.umich.edu/stories/anhong-guo-receives-google-funding-for-live-visual-description-platform-for-blind-people" rel="external nofollow noopener" target="_blank">WorldScribe</a> received <a href="https://research.google/programs-and-events/google-academic-research-awards/" rel="external nofollow noopener" target="_blank">Google Academic Award (Socity-Centered AI)</a>! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/worldscribe2-480.webp 480w,/assets/img/publication_preview/worldscribe2-800.webp 800w,/assets/img/publication_preview/worldscribe2-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/worldscribe2.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="worldscribe2.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3654777.3676375" class="col-sm-8"> <div class="title">WorldScribe: Towards Context-Aware Live Visual Descriptions</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Yuxuan Liu,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô24)</b>, Pittsburgh, PA, USA, 2024 </div> <div class="periodical"> </div> <div class="award">üèÜ Best Paper Award (Top 1%)</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://worldscribe.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/worldscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/9dRExJzyqxw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.youtube.com/watch?v=zpN85oFrSOE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">30s Preview</a> </div> <div class="abstract hidden"> <p>Automated live visual descriptions can aid blind people in understanding their surroundings with autonomy and independence. However, providing descriptions that are rich, contextual, and just-in-time has been a long-standing challenge in accessibility. In this work, we develop WorldScribe, a system that generates automated live real-world visual descriptions that are customizable and adaptive to users‚Äô contexts: (i) WorldScribe‚Äôs descriptions are tailored to users‚Äô intents and prioritized based on semantic relevance. (ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively succinct descriptions for dynamic scenes, while presenting longer and detailed ones for stable settings. (iii) WorldScribe is adaptive to sound contexts, e.g., increasing volume in noisy environments, or pausing when conversations start. Powered by a suite of vision, language, and sound recognition models, WorldScribe introduces a description generation pipeline that balances the tradeoffs between their richness and latency to support real-time use. The design of WorldScribe is informed by prior work on providing visual descriptions and a formative study with blind participants. Our user study and subsequent pipeline evaluation show that WorldScribe can provide real-time and fairly accurate visual descriptions to facilitate environment understanding that is adaptive and customized to users‚Äô contexts. Finally, we discuss the implications and further steps toward making live visual descriptions more context-aware and humanized.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/editscribe-480.webp 480w,/assets/img/publication_preview/editscribe-800.webp 800w,/assets/img/publication_preview/editscribe-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/editscribe.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="editscribe.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3663548.3675599" class="col-sm-8"> <div class="title">EditScribe: Non-Visual Image Editing with Natural Language Verification Loops</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Yuxuan Liu,¬†Lotus Zhang,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility</em><b> (ASSETS ‚Äô24)</b>, St. John‚Äôs, Newfoundland and Labrador, Canada, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/editscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=x9Kk-nT-yfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes image editing accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/soundblender-480.webp 480w,/assets/img/publication_preview/soundblender-800.webp 800w,/assets/img/publication_preview/soundblender-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/soundblender.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="soundblender.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3643834.3661556" class="col-sm-8"> <div class="title">SoundShift: Exploring Sound Manipulations for Accessible Mixed-Reality Awareness</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chia-Sheng Hung,¬†Bing-Yu Chen,¬†Dhruv Jain,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 2024 ACM Designing Interactive Systems Conference</em><b> (DIS ‚Äô24)</b>, IT University of Copenhagen, Denmark, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/soundshift.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=HYtQlb__Df8&amp;ab_channel=Ruei-CheChang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Mixed-reality (MR) soundscapes blend real-world sound with virtual audio from hearing devices, presenting intricate auditory information that is hard to discern and differentiate. This is particularly challenging for blind or visually impaired individuals, who rely on sounds and descriptions in their everyday lives. To understand how complex audio information is consumed, we analyzed online forum posts within the blind community, identifying prevailing challenges, needs, and desired solutions. We synthesized the results and propose SoundShift for increasing MR sound awareness, which includes six sound manipulations: Transparency Shift, Envelope Shift, Position Shift, Style Shift, Time Shift, and Sound Append. To evaluate the effectiveness of SoundShift, we conducted a user study with 18 blind participants across three simulated MR scenarios, where participants identified specific sounds within intricate soundscapes. We found that SoundShift increased MR sound awareness and minimized cognitive load. Finally, we developed three real-world example applications to demonstrate the practicality of SoundShift.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/omniscribe-480.webp 480w,/assets/img/publication_preview/omniscribe-800.webp 800w,/assets/img/publication_preview/omniscribe-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/omniscribe.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="omniscribe.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3526113.3545613" class="col-sm-8"> <div class="title">OmniScribe: Authoring Immersive Audio Descriptions for 360¬∞ Videos</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chao-Hsien Ting,¬†Chia-Sheng Hung,¬†Wan-Chen Lee,¬†Liang-Jin Chen,¬†Yu-Tzu Chao,¬†Bing-Yu Chen,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô22)</b>, Bend, OR, USA, 2022 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://omniscribe.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/omniscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=JL4Zuw7Hr6U&amp;ab_channel=Ruei-CheChang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360¬∞ video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360¬∞ videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360¬∞ videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360¬∞ videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360¬∞ video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360¬∞ videos. Finally, we discuss the implications of promoting 360¬∞ video accessibility.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Ruei-Che Chang. Last updated: March 26, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-3P6TGY38D9"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-3P6TGY38D9");</script> </body> </html>