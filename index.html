<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ruei-Che Chang ÂºµÁùøÂì≤ </title> <meta name="author" content="Ruei-Che Chang ÂºµÁùøÂì≤"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rueiche.com//"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/rueichechangcv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Ruei-Che</span> Chang ÂºµÁùøÂì≤ </h1> <p class="desc">Ph.D. Candidate @ University of Michigan, Ann Arbor</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rueiche-480.webp 480w,/assets/img/rueiche-800.webp 800w,/assets/img/rueiche-1400.webp 1400w," sizes="(min-width: 810px) 234.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/rueiche.png?5713026d785521a667adc5a30fedd34d" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="rueiche.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="mailto:rueiche@umich.edu"><i class="fa-solid fa-envelope"></i> <u>rueiche@umich.edu</u></a><br> <a href="https://www.linkedin.com/in/ruei-che-chang-65b553166/" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i> Linkedin</a><br> <a href="https://x.com/RueiChe" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-twitter"></i> Twitter</a><br> <a href="https://scholar.google.com.tw/citations?user=G14TzOEAAAAJ&amp;hl=zh-TW" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-google-scholar"></i> Google Scholar</a><br> <a href="../assets/pdf/rueichechangcv.pdf"><i class="fa-solid fa-file-pdf"></i> Curriculum Vitae</a><br> </div> </div> <div class="clearfix"> <p>I am a part of Human-AI Lab at <a href="https://cse.engin.umich.edu/" rel="external nofollow noopener" target="_blank">UMich CSE</a>, advised by <a href="https://guoanhong.com/" rel="external nofollow noopener" target="_blank">Anhong Guo</a>. My research is supported by the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2025" rel="external nofollow noopener" target="_blank">Ô£ø Apple Scholars in AIML PhD Fellowship</a>, <a href="https://cse.engin.umich.edu/stories/ruei-che-chang-receives-rackham-international-student-fellowship" rel="external nofollow noopener" target="_blank">Rackham International Students Fellowship</a>, and Weinberg Cognitive Science Fellowship.</p> <p>My research builds interactive AI systems for human-level assistance in real-world tasks. I aim to build an AI companion that can see, hear, and remember past interactions and environments, so it can understand users over time and proactively provide timely, personalized support.</p> <p>During my PhD study, I interned at <a href="https://about.meta.com/realitylabs/" rel="external nofollow noopener" target="_blank">Meta Reality Labs</a> and <a href="https://research.adobe.com/" rel="external nofollow noopener" target="_blank">Adobe Research</a>. Prior to that, I graduated with a Master degree in Computer Science at <a href="https://web.cs.dartmouth.edu/" rel="external nofollow noopener" target="_blank">Dartmouth College</a> and a Bachelor degree in Electrical Engineering at <a href="https://www.ee.ncku.edu.tw/en/" rel="external nofollow noopener" target="_blank">National Cheng Kung University</a> in Taiwan.</p> </div> <div class="about-news-section"> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%"> <span style="color: #f92080; font-weight: bold;">Feb 07, 2026</span> </th> <td> <a href="assets/pdf/touchscribe.pdf" style="color: #f92080; font-weight: bold;">TouchScribe is accepted to CHI‚Äô26, see you in üá™üá∏ Barcelona!</a> </td> </tr> <tr> <th scope="row" style="width: 20%"> Oct 30, 2025 </th> <td> Excited to share that I will be interning at Apple Ô£ø in Seattle this summer! </td> </tr> <tr> <th scope="row" style="width: 20%"> Aug 05, 2025 </th> <td> I will attend both <a href="https://uist.acm.org/2025/cfp/#doctoral-symposium" rel="external nofollow noopener" target="_blank">UIST DS 2025</a> and <a href="https://assets25.sigaccess.org/doctoral_consortium.html" rel="external nofollow noopener" target="_blank">ASSETS DC 2025</a>! </td> </tr> <tr> <th scope="row" style="width: 20%"> Aug 04, 2025 </th> <td> findings on exploring <a href="assets/pdf/chatgpt.pdf">ChatGPT‚Äôs Live Video Chat for Real-World Assistance</a> was accepted to <a href="https://assets25.sigaccess.org/" rel="external nofollow noopener" target="_blank">ASSETS 2025</a>! </td> </tr> <tr> <th scope="row" style="width: 20%"> Aug 04, 2025 </th> <td> my Meta intern project <a href="assets/pdf/viago.pdf">Viago</a> was accepted to <a href="https://uist.acm.org/2025/" rel="external nofollow noopener" target="_blank">UIST 2025</a>! </td> </tr> </table> </div> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/touchscribe-480.webp 480w,/assets/img/publication_preview/touchscribe-800.webp 800w,/assets/img/publication_preview/touchscribe-1400.webp 1400w," sizes="220px" type="image/webp"> <img src="/assets/img/publication_preview/touchscribe.gif" class="preview z-depth-1 rounded" width="100%" height="auto" style=" max-width: 220px; " alt="touchscribe.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3772318.3791308" class="col-sm-7" style="padding-left: 0.3rem;"> <div class="title">TouchScribe: Augmenting Non-Visual Hand-Object Interactions with Automated Live Visual Descriptions</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Rosiana Natalie,¬†Wenqian Xu,¬†Jovan Zheng Feng Yap,¬†Tiange Luo,¬†Venkatesh Potluri,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems</em><b> (CHI ‚Äô26)</b>, Barcelona, Spain, 2026 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3772318.3791308" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a href="https://www.arxiv.org/abs/2602.07802" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/touchscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/-sX-9W8DD4M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>People who are blind or have low vision regularly use their hands to interact with the physical world to gain access to objects‚Äô shape, size, weight, and texture. However, many rich visual features remain inaccessible through touch alone, making it difficult to distinguish similar objects, interpret visual affordances, and form a complete understanding of objects. In this work, we present TouchScribe, a system that augments hand-object interactions with automated live visual descriptions. We trained a custom egocentric hand interaction model to recognize both common gestures (e.g., grab to inspect, hold side-by-side to compare) and unique ones by blind people (e.g., point to explore color, or swipe to read available texts). Furthermore, TouchScribe provides real-time and adaptive feedback based on hand movement, from hand interaction states, to object labels, and to visual details. Our user study and technical evaluations demonstrate that TouchScribe can provide rich and useful descriptions to support object understanding. Finally, we discuss the implications of making live visual descriptions responsive to users‚Äô physical reach.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/worldscribe2-480.webp 480w,/assets/img/publication_preview/worldscribe2-800.webp 800w,/assets/img/publication_preview/worldscribe2-1400.webp 1400w," sizes="220px" type="image/webp"> <img src="/assets/img/publication_preview/worldscribe2.gif" class="preview z-depth-1 rounded" width="100%" height="auto" style=" max-width: 220px; " alt="worldscribe2.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3654777.3676375" class="col-sm-7" style="padding-left: 0.3rem;"> <div class="title">WorldScribe: Towards Context-Aware Live Visual Descriptions</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Yuxuan Liu,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô24)</b>, Pittsburgh, PA, USA, 2024 </div> <div class="periodical"> </div> <div class="award">üèÜ Best Paper Award (Top 1%)</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3654777.3676375" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a href="https://arxiv.org/abs/2408.06627" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/worldscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=zpN85oFrSOE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Preview</a> <a href="https://youtu.be/9dRExJzyqxw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://worldscribe.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://news.engin.umich.edu/2024/10/real-time-descriptions-of-surroundings-for-people-who-are-blind/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Press</a> </div> <div class="abstract hidden"> <p>Automated live visual descriptions can aid blind people in understanding their surroundings with autonomy and independence. However, providing descriptions that are rich, contextual, and just-in-time has been a long-standing challenge in accessibility. In this work, we develop WorldScribe, a system that generates automated live real-world visual descriptions that are customizable and adaptive to users‚Äô contexts: (i) WorldScribe‚Äôs descriptions are tailored to users‚Äô intents and prioritized based on semantic relevance. (ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively succinct descriptions for dynamic scenes, while presenting longer and detailed ones for stable settings. (iii) WorldScribe is adaptive to sound contexts, e.g., increasing volume in noisy environments, or pausing when conversations start. Powered by a suite of vision, language, and sound recognition models, WorldScribe introduces a description generation pipeline that balances the tradeoffs between their richness and latency to support real-time use. The design of WorldScribe is informed by prior work on providing visual descriptions and a formative study with blind participants. Our user study and subsequent pipeline evaluation show that WorldScribe can provide real-time and fairly accurate visual descriptions to facilitate environment understanding that is adaptive and customized to users‚Äô contexts. Finally, we discuss the implications and further steps toward making live visual descriptions more context-aware and humanized.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ds-480.webp 480w,/assets/img/publication_preview/ds-800.webp 800w,/assets/img/publication_preview/ds-1400.webp 1400w," sizes="220px" type="image/webp"> <img src="/assets/img/publication_preview/ds.png" class="preview z-depth-1 rounded" width="100%" height="auto" style=" max-width: 220px; " alt="ds.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3746058.3758468" class="col-sm-7" style="padding-left: 0.3rem;"> <div class="title">Enabling Real-World Assistive Agents: From Live Vision to Proactive Context-Aware Information Delivery</div> <div class="author"> <em>Ruei-Che Chang</em> </div> <div class="periodical"> <em>In Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST Adjunct ‚Äô25)</b>, Busan, Korea, 2025 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3746058.3758468" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a href="/assets/pdf/ds.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Interacting with the real world is a fundamental part of daily life, yet it remains challenging for individuals who are blind or visually impaired (BVI). It demands live, contextual understanding of dynamic environments, along with interactive, multimodal communication to fulfill their sensory and cognitive needs. To address this, my dissertation develops assistive AI systems and frameworks that observe the real world through multimodal sensing, reason about essential information in response to user contexts, and deliver human-like verbal communication to support real-world understanding. First, I explored the design insights of assistive AI agents by investigating how BVI users interact with human-like video AI systems across diverse real-world contexts. Second, with the identified insights, such as a lack of proactivity, I developed a mobile application that analyzes live camera feeds to generate real-time visual descriptions aligned with user goals, delivering them in harmony with the audio environment. Lastly, I extend it with a set of human-like capabilities, such as memory, spatial understanding, and the ability to infer intent from natural interactions, to act as a long-term assistive companion. Ultimately, my dissertation advances a paradigm shift from digital agents to real-world assistive agents that enhance the independence and agency of BVI individuals.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/chatgpt2-480.webp 480w,/assets/img/publication_preview/chatgpt2-800.webp 800w,/assets/img/publication_preview/chatgpt2-1400.webp 1400w," sizes="220px" type="image/webp"> <img src="/assets/img/publication_preview/chatgpt2.png" class="preview z-depth-1 rounded" width="100%" height="auto" style=" max-width: 220px; " alt="chatgpt2.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="liveVIdeo" class="col-sm-7" style="padding-left: 0.3rem;"> <div class="title">Probing the Gaps in ChatGPT‚Äôs Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Rosiana Natalie,¬†Wenqian Xu,¬†Jovan Zheng Feng Yap,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 27th International ACM SIGACCESS Conference on Computers and Accessibility</em><b> (ASSETS ‚Äô25)</b>, , 2025 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3663547.3746319" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a href="https://arxiv.org/abs/2508.03651" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/chatgpt.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPT‚Äôs Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about users‚Äô visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/editscribe-480.webp 480w,/assets/img/publication_preview/editscribe-800.webp 800w,/assets/img/publication_preview/editscribe-1400.webp 1400w," sizes="220px" type="image/webp"> <img src="/assets/img/publication_preview/editscribe.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" style=" max-width: 220px; " alt="editscribe.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3663548.3675599" class="col-sm-7" style="padding-left: 0.3rem;"> <div class="title">EditScribe: Non-Visual Image Editing with Natural Language Verification Loops</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Yuxuan Liu,¬†Lotus Zhang,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility</em><b> (ASSETS ‚Äô24)</b>, St. John‚Äôs, Newfoundland and Labrador, Canada, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3663548.3675599" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a href="https://arxiv.org/abs/2408.06632" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/editscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=x9Kk-nT-yfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes image editing accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/omniscribe-480.webp 480w,/assets/img/publication_preview/omniscribe-800.webp 800w,/assets/img/publication_preview/omniscribe-1400.webp 1400w," sizes="220px" type="image/webp"> <img src="/assets/img/publication_preview/omniscribe.gif" class="preview z-depth-1 rounded" width="100%" height="auto" style=" max-width: 220px; " alt="omniscribe.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3526113.3545613" class="col-sm-7" style="padding-left: 0.3rem;"> <div class="title">OmniScribe: Authoring Immersive Audio Descriptions for 360¬∞ Videos</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chao-Hsien Ting,¬†Chia-Sheng Hung,¬†Wan-Chen Lee,¬†Liang-Jin Chen,¬†Yu-Tzu Chao,¬†Bing-Yu Chen,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô22)</b>, Bend, OR, USA, 2022 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3526113.3545613" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACM</a> <a href="/assets/pdf/omniscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=JL4Zuw7Hr6U&amp;ab_channel=Ruei-CheChang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://omniscribe.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360¬∞ video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360¬∞ videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360¬∞ videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360¬∞ videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360¬∞ video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360¬∞ videos. Finally, we discuss the implications of promoting 360¬∞ video accessibility.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Ruei-Che Chang ÂºµÁùøÂì≤. Last updated: February 23, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-3P6TGY38D9"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-3P6TGY38D9");</script> </body> </html>