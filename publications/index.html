<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Ruei-Che Chang </title> <meta name="author" content="Ruei-Che Chang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rueichechang.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ruei-Che</span> Chang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/rueichechangcv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/worldscribe2-480.webp 480w,/assets/img/publication_preview/worldscribe2-800.webp 800w,/assets/img/publication_preview/worldscribe2-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/worldscribe2.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="worldscribe2.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3654777.3676375" class="col-sm-8"> <div class="title">WorldScribe: Towards Context-Aware Live Visual Descriptions</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Yuxuan Liu,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô24)</b>, Pittsburgh, PA, USA, 2024 </div> <div class="periodical"> </div> <div class="award">üèÜ Best Paper Award (Top 1%)</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://worldscribe.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/worldscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/9dRExJzyqxw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.youtube.com/watch?v=zpN85oFrSOE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">30s Preview</a> </div> <div class="abstract hidden"> <p>Automated live visual descriptions can aid blind people in understanding their surroundings with autonomy and independence. However, providing descriptions that are rich, contextual, and just-in-time has been a long-standing challenge in accessibility. In this work, we develop WorldScribe, a system that generates automated live real-world visual descriptions that are customizable and adaptive to users‚Äô contexts: (i) WorldScribe‚Äôs descriptions are tailored to users‚Äô intents and prioritized based on semantic relevance. (ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively succinct descriptions for dynamic scenes, while presenting longer and detailed ones for stable settings. (iii) WorldScribe is adaptive to sound contexts, e.g., increasing volume in noisy environments, or pausing when conversations start. Powered by a suite of vision, language, and sound recognition models, WorldScribe introduces a description generation pipeline that balances the tradeoffs between their richness and latency to support real-time use. The design of WorldScribe is informed by prior work on providing visual descriptions and a formative study with blind participants. Our user study and subsequent pipeline evaluation show that WorldScribe can provide real-time and fairly accurate visual descriptions to facilitate environment understanding that is adaptive and customized to users‚Äô contexts. Finally, we discuss the implications and further steps toward making live visual descriptions more context-aware and humanized.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/editscribe-480.webp 480w,/assets/img/publication_preview/editscribe-800.webp 800w,/assets/img/publication_preview/editscribe-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/editscribe.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="editscribe.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3663548.3675599" class="col-sm-8"> <div class="title">EditScribe: Non-Visual Image Editing with Natural Language Verification Loops</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Yuxuan Liu,¬†Lotus Zhang,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility</em><b> (ASSETS ‚Äô24)</b>, St. John‚Äôs, Newfoundland and Labrador, Canada, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/editscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=x9Kk-nT-yfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Image editing is an iterative process that requires precise visual evaluation and manipulation for the output to match the editing intent. However, current image editing tools do not provide accessible interaction nor sufficient feedback for blind and low vision individuals to achieve this level of control. To address this, we developed EditScribe, a prototype system that makes image editing accessible using natural language verification loops powered by large multimodal models. Using EditScribe, the user first comprehends the image content through initial general and object descriptions, then specifies edit actions using open-ended natural language prompts. EditScribe performs the image edit, and provides four types of verification feedback for the user to verify the performed edit, including a summary of visual changes, AI judgement, and updated general and object descriptions. The user can ask follow-up questions to clarify and probe into the edits or verification feedback, before performing another edit. In a study with ten blind or low-vision users, we found that EditScribe supported participants to perform and verify image edit actions non-visually. We observed different prompting strategies from participants, and their perceptions on the various types of verification feedback. Finally, we discuss the implications of leveraging natural language verification loops to make visual authoring non-visually accessible.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/customad-480.webp 480w,/assets/img/publication_preview/customad-800.webp 800w,/assets/img/publication_preview/customad-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/customad.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="customad.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3663548.3675617" class="col-sm-8"> <div class="title">Audio Description Customization</div> <div class="author"> Rosiana Natalie,¬†<em>Ruei-Che Chang</em>,¬†Smitha Sheshadri,¬†Anhong Guo,¬†and¬†Kotaro Hara </div> <div class="periodical"> <em>In Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility</em><b> (ASSETS ‚Äô24)</b>, St. John‚Äôs, NL, Canada, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/customad.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=Ijb3QeOdkhI&amp;ab_channel=AnhongGuo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Blind and low-vision (BLV) people use audio descriptions (ADs) to access videos. However, current ADs are unalterable by end users, thus are incapable of supporting BLV individuals‚Äô potentially diverse needs and preferences. This research investigates if customizing AD could improve how BLV individuals consume videos. We conducted an interview study (Study 1) with fifteen BLV participants, which revealed desires for customizing properties like length, emphasis, speed, voice, format, tone, and language. At the same time, concerns like interruptions and increased interaction load due to customization emerged. To examine AD customization‚Äôs effectiveness and tradeoffs, we designed CustomAD, a prototype that enables BLV users to customize AD content and presentation. An evaluation study (Study 2) with twelve BLV participants showed using CustomAD significantly enhanced BLV people‚Äôs video understanding, immersion, and information navigation efficiency. Our work illustrates the importance of AD customization and offers a design that enhances video accessibility for BLV individuals.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/soundblender-480.webp 480w,/assets/img/publication_preview/soundblender-800.webp 800w,/assets/img/publication_preview/soundblender-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/soundblender.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="soundblender.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3643834.3661556" class="col-sm-8"> <div class="title">SoundShift: Exploring Sound Manipulations for Accessible Mixed-Reality Awareness</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chia-Sheng Hung,¬†Bing-Yu Chen,¬†Dhruv Jain,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 2024 ACM Designing Interactive Systems Conference</em><b> (DIS ‚Äô24)</b>, IT University of Copenhagen, Denmark, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/soundshift.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=HYtQlb__Df8&amp;ab_channel=Ruei-CheChang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Mixed-reality (MR) soundscapes blend real-world sound with virtual audio from hearing devices, presenting intricate auditory information that is hard to discern and differentiate. This is particularly challenging for blind or visually impaired individuals, who rely on sounds and descriptions in their everyday lives. To understand how complex audio information is consumed, we analyzed online forum posts within the blind community, identifying prevailing challenges, needs, and desired solutions. We synthesized the results and propose SoundShift for increasing MR sound awareness, which includes six sound manipulations: Transparency Shift, Envelope Shift, Position Shift, Style Shift, Time Shift, and Sound Append. To evaluate the effectiveness of SoundShift, we conducted a user study with 18 blind participants across three simulated MR scenarios, where participants identified specific sounds within intricate soundscapes. We found that SoundShift increased MR sound awareness and minimized cognitive load. Finally, we developed three real-world example applications to demonstrate the practicality of SoundShift.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/imageexplorer-480.webp 480w,/assets/img/publication_preview/imageexplorer-800.webp 800w,/assets/img/publication_preview/imageexplorer-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/imageexplorer.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="imageexplorer.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3677846.3677861" class="col-sm-8"> <div class="title">ImageExplorer Deployment: Understanding Text-Based and Touch-Based Image Exploration in the Wild</div> <div class="author"> Andi Xu,¬†Minyu Cai,¬†Dier Hou,¬†<em>Ruei-Che Chang</em>,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 21st International Web for All Conference</em><b> (W4A ‚Äô24)</b>, Singapore, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://imageexplorer.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/imageexplorer.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=6iSkLlNRDuo&amp;ab_channel=AnhongGuo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Blind and visually-impaired (BVI) users often rely on alt-texts to understand images. AI-generated alt-texts can be scalable and efficient but may lack details and are prone to errors. Multi-layered touch interfaces, on the other hand, can provide rich details and spatial information, but may take longer to explore and cause higher mental load. To understand how BVI users leverage these two methods, we deployed ImageExplorer, an iOS app on the Apple App Store that provides multi-layered image information via both text-based and touch-based interfaces with customizable levels of granularity. Across 12 months, 371 users uploaded 651 images and explored 694 times. Their activities were logged to help us understand how BVI users consume image captions in the wild. This work informs a holistic understanding of BVI users‚Äô image exploration behavior and influential factors. We provide design implications for future models of image captioning and visual access tools.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/advcaptcha-480.webp 480w,/assets/img/publication_preview/advcaptcha-800.webp 800w,/assets/img/publication_preview/advcaptcha-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/advcaptcha.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="advcaptcha.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="leeadvcaptcha" class="col-sm-8"> <div class="title">AdvCAPTCHA: Creating Usable and Secure Audio CAPTCHA with Adversarial Machine Learning</div> <div class="author"> Hao-Ping Hank Lee,¬†Wei-Lun Kao,¬†Hung-Jui Wang,¬†<em>Ruei-Che Chang</em>,¬†Yi-Hao Peng,¬†Fu-Yin Cherng,¬†and¬†Shang-Tse Chen </div> <div class="periodical"> <em></em><b> (USEC‚Äô24)</b>, San Diego, CA, USA, 2024 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ndss-symposium.org/wp-content/uploads/usec2024-35-paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Audio CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) is an accessible alternative to the traditional CAPTCHA for people with visual impairments. However, the literature has found that audio CAPTCHA suffers from both lower usability and security than its visual counterpart. In this paper, we propose AdvCAPTCHA, a novel audio CAPTCHA generated by using adversarial machine learning techniques. By conducting studies with people with and without visual impairments, we show that AdvCAPTCHA can outperform the \textitstatus quo audio CAPTCHA in security but not usability. We demonstrate AdvCAPTCHA‚Äôs feasibility of providing detection of malicious attacks. We also present an evaluation metric, \textitthresholding, to quantify the trade-off between usability and security for audio CAPTCHA design. Finally, we discuss approaches to the real-world adoption of AdvCAPTCHA.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lasercut-480.webp 480w,/assets/img/publication_preview/lasercut-800.webp 800w,/assets/img/publication_preview/lasercut-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/lasercut.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lasercut.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3544548.3580684" class="col-sm-8"> <div class="title">Understanding (Non-)Visual Needs for the Design of Laser-Cut Models</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Seraphina Yong,¬†Fang-Ying Liao,¬†Chih-An Tsao,¬†and¬†Bing-Yu Chen </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em><b> (CHI ‚Äô23)</b>, Hamburg, Germany, 2023 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/chi23-lasercut-study.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=5eBeNT0fctU&amp;t=2s&amp;ab_channel=ACMSIGCHI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Laser-cutting is a promising fabrication method that empowers makers, including blind or visually-impaired (BVI) creators, to create technologies that fit their needs. Existing work on laser-cut accessibility has facilitated easier assembly as a workaround for existing models. However, laser-cut models are still not designed to accommodate the needs of BVI users. Integrating BVI needs can enrich the greater maker community by enabling cross-group discourse on laser-cut making. To investigate how laser-cut model design can be more accessible overall, we study laser-cut assembly as a process deeply intertwined with the fundamental design of laser-cut models. We present a study with seven sighted and seven BVI participants to compare their usage of laser-cut model affordances during assembly. Data for the BVI participants in this study originate from a previous work [13]. We identify assembly cues common or unique to sighted and BVI users, and discuss implications to improve general accessibility in laser-cut design.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/omniscribe-480.webp 480w,/assets/img/publication_preview/omniscribe-800.webp 800w,/assets/img/publication_preview/omniscribe-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/omniscribe.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="omniscribe.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3526113.3545613" class="col-sm-8"> <div class="title">OmniScribe: Authoring Immersive Audio Descriptions for 360¬∞ Videos</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chao-Hsien Ting,¬†Chia-Sheng Hung,¬†Wan-Chen Lee,¬†Liang-Jin Chen,¬†Yu-Tzu Chao,¬†Bing-Yu Chen,¬†and¬†Anhong Guo </div> <div class="periodical"> <em>In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô22)</b>, Bend, OR, USA, 2022 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://omniscribe.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/omniscribe.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=JL4Zuw7Hr6U&amp;ab_channel=Ruei-CheChang" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Blind people typically access videos via audio descriptions (AD) crafted by sighted describers who comprehend, select, and describe crucial visual content in the videos. 360¬∞ video is an emerging storytelling medium that enables immersive experiences that people may not possibly reach in everyday life. However, the omnidirectional nature of 360¬∞ videos makes it challenging for describers to perceive the holistic visual content and interpret spatial information that is essential to create immersive ADs for blind people. Through a formative study with a professional describer, we identified key challenges in describing 360¬∞ videos and iteratively designed OmniScribe, a system that supports the authoring of immersive ADs for 360¬∞ videos. OmniScribe uses AI-generated content-awareness overlays for describers to better grasp 360¬∞ video content. Furthermore, OmniScribe enables describers to author spatial AD and immersive labels for blind users to consume the videos immersively with our mobile prototype. In a study with 11 professional and novice describers, we demonstrated the value of OmniScribe in the authoring workflow; and a study with 8 blind participants revealed the promise of immersive AD over standard AD for 360¬∞ videos. Finally, we discuss the implications of promoting 360¬∞ video accessibility.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/puppeteer-480.webp 480w,/assets/img/publication_preview/puppeteer-800.webp 800w,/assets/img/publication_preview/puppeteer-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/puppeteer.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="puppeteer.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3562939.3565609" class="col-sm-8"> <div class="title">Puppeteer: Exploring Intuitive Hand Gestures and Upper-Body Postures for Manipulating Human Avatar Actions</div> <div class="author"> Ching-Wen Hung,¬†<em>Ruei-Che Chang</em>,¬†Hong-Sheng Chen,¬†Chung Han Liang,¬†Liwei Chan,¬†and¬†Bing-Yu Chen </div> <div class="periodical"> <em>In Proceedings of the 28th ACM Symposium on Virtual Reality Software and Technology</em><b> (VRST ‚Äô22)</b>, Tsukuba, Japan, 2022 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/puppeteer.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=vhLszbgKaYw&amp;ab_channel=Ching-WenHung%28HOSI%29" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Body-controlled avatars provide a more intuitive method to real-time control virtual avatars but require larger environment space and more user effort. In contrast, hand-controlled avatars give more dexterous and fewer fatigue manipulations within a close-range space for avatar control but provide fewer sensory cues than the body-based method. This paper investigates the differences between the two manipulations and explores the possibility of a combination. We first performed a formative study to understand when and how users prefer manipulating hands and bodies to represent avatars‚Äô actions in current popular video games. Based on the top video games survey, we decided to represent human avatars‚Äô motions. Besides, we found that players used their bodies to represent avatar actions but changed to using hands when they were too unrealistic and exaggerated to mimic by bodies (e.g., flying in the sky, rolling over quickly). Hand gestures also provide an alternative to lower-body motions when players want to sit during gaming and do not want extensive effort to move their avatars. Hence, we focused on the design of hand gestures and upper-body postures. We present Puppeteer, an input prototype system that allows players directly control their avatars through intuitive hand gestures and upper-body postures. We selected 17 avatar actions discovered in the formative study and conducted a gesture elicitation study to invite 12 participants to design best representing hand gestures and upper-body postures for each action. Then we implemented a prototype system using the MediaPipe framework to detect keypoints and a self-trained model to recognize 17 hand gestures and 17 upper-body postures. Finally, three applications demonstrate the interactions enabled by Puppeteer.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/daedalus-480.webp 480w,/assets/img/publication_preview/daedalus-800.webp 800w,/assets/img/publication_preview/daedalus-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/daedalus.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="daedalus.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3472749.3474754" class="col-sm-8"> <div class="title">Daedalus in the Dark: Designing for Non-Visual Accessible Construction of Laser-Cut Architecture</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chih-An Tsao,¬†Fang-Ying Liao,¬†Seraphina Yong,¬†Tom Yeh,¬†and¬†Bing-Yu Chen </div> <div class="periodical"> <em>In The 34th Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô21)</b>, Virtual Event, USA, 2021 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/daedalus.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=0LE2zqfwbYw&amp;ab_channel=ACMSIGCHI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Design tools and research regarding laser-cut architectures have been widely explored in the past decade. However, such discussion has mostly revolved around technical and structural design questions instead of another essential element of laser-cut models ‚Äî assembly ‚Äî a process that relies heavily on components‚Äô visual affordance, therefore less accessible to blind or low vision (BLV) people. To narrow the gap in this area, we co-designed with 7 BLV people to examine their assembly experience with different laser-cut architectures. From their feedback, we proposed several design heuristics and guidelines for Daedalus, a generative design tool that can produce tactile aids for laser-cut assembly given a few high-level manual inputs. We validate the proposed aids in a user study with 8 new BLV participants. Our results revealed that BLV users can manage laser-cut assembly more efficiently with Daedalus. Going forth from this design iteration, we discuss implications for future research on accessible laser-cut assembly.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/AC-480.webp 480w,/assets/img/publication_preview/AC-800.webp 800w,/assets/img/publication_preview/AC-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/AC.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="AC.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3411764.3445690" class="col-sm-8"> <div class="title">AccessibleCircuits: Adaptive Add-On Circuit Components for People with Blindness or Low Vision</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Wen-Ping Wang,¬†Chi-Huan Chiang,¬†Te-Yen Wu,¬†Zheer Xu,¬†Justin Luo,¬†Bing-Yu Chen,¬†and¬†Xing-Dong Yang </div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em><b> (CHI ‚Äô21)</b>, Yokohama, Japan, 2021 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/accessiblecircuits.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=ROeoMCWjFsc&amp;ab_channel=HCIResearch%40SFU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>In this paper, we propose the designs for low cost and 3D-printable add-on components to adapt existing breadboards, circuit components and electronics tools for blind or low vision (BLV) users. Through an initial user study, we identified several barriers to entry for beginners with BLV in electronics and circuit prototyping. These barriers guided the design and development of our add-on components. We focused on developing adaptations that provide additional information about the specific component pins and breadboard holes, modify tools to make them easier to use for users with BLV, and expand non-visual feedback (e.g., audio, tactile) for tasks that require vision. Through a second user study, we demonstrated that our adaptations can effectively overcome the accessibility barriers in breadboard circuit prototyping for users with BLV.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/glissade-480.webp 480w,/assets/img/publication_preview/glissade-800.webp 800w,/assets/img/publication_preview/glissade-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/glissade.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="glissade.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3313831.3376505" class="col-sm-8"> <div class="title">Glissade: Generating Balance Shifting Feedback to Facilitate Auxiliary Digital Pen Input</div> <div class="author"> Kai-Chieh Huang,¬†Chen-Kuo Sun,¬†Da-Yuan Huang,¬†Yu-Chun Chen,¬†<em>Ruei-Che Chang</em>,¬†Shuo-wen Hsu,¬†Chih-Yun Yang,¬†and¬†Bing-Yu Chen </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em><b> (CHI ‚Äô20)</b>, Honolulu, HI, USA, 2020 </div> <div class="periodical"> </div> <div class="award">üèÖ Best Paper Honorable Mention Award</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/glissade.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=feZxCi5I_fE&amp;ab_channel=ACMSIGCHI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>This paper introduces Glissade, a digital pen that generates balance shifting feedback by changing the weight distribution of the pen. A pulley system shifts a brass mass inside the pen to change the pen‚Äôs center of mass and moment of inertia. When the mass is stationary, the pen delivers a constant yet natural sensation of weight, which can be used to convey a status. The pen can also generate a variety of haptic clues by actuating the mass according to the tilt or rotation of the pen, two commonly-used auxiliary pen input channels. Glissade demonstrates new possibilities that balance shifting feedback can bring to digital pen interactions. We validated the usability of this feedback by determining the recognizability of six balance patterns ‚Äì a mix of static and dynamic patterns chosen based on our design considerations ‚Äì in two controlled experiments. The results show that, on average, the participants could distinguish between the patterns with a 94.25% accuracy. At the end, we demonstrate a set of novel interactions enabled by Glissade and discuss the directions for future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/SHRA-480.webp 480w,/assets/img/publication_preview/SHRA-800.webp 800w,/assets/img/publication_preview/SHRA-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/SHRA.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SHRA.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3313831.3376501" class="col-sm-8"> <div class="title">Exploring the Design Space of User-System Communication for Smart-home Routine Assistants</div> <div class="author"> Yi-Shyuan Chiang,¬†<em>Ruei-Che Chang</em>,¬†Yi-Lin Chuang,¬†Shih-Ya Chou,¬†Hao-Ping Lee,¬†I-Ju Lin,¬†Jian-Hua Jiang Chen,¬†and¬†Yung-Ju Chang </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em><b> (CHI ‚Äô20)</b>, Honolulu, HI, USA, 2020 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/shra.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=UhrUftPhTDQ&amp;ab_channel=ACMSIGCHI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>AI-enabled smart-home agents that automate household routines are increasingly viable, but the design space of how and what such systems should communicate with their users remains underexplored. Through a user-enactment study, we identified various interpretations of and feelings toward such a system‚Äôs confidence in its automated acts. That confidence and their own mental models influenced what and how the participants wanted the system to communicate, as well as how they would assess, diagnose, and subsequently improve it. Automated acts resulted from false predictions were not generally considered improper, provided that they were perceived as reasonable or potentially useful. The participants‚Äô improvement strategies were of four general types, all of which will be discussed. Factors affecting their preferred levels of involvement in automated acts and their interest in system confidence were also identified. We conclude by making practical design recommendations for the user-system communication design spaces of smart-home routine assistants.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tango-480.webp 480w,/assets/img/publication_preview/tango-800.webp 800w,/assets/img/publication_preview/tango-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/tango.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tango.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3385959.3418457" class="col-sm-8"> <div class="title">TanGo: Exploring Expressive Tangible Interactions on Head-Mounted Displays</div> <div class="author"> <em>Ruei-Che Chang</em>,¬†Chi-Huan Chiang,¬†Shuo-wen Hsu,¬†Chih-Yun Yang,¬†Da-Yuan Huang,¬†and¬†Bing-Yu Chen </div> <div class="periodical"> <em>In Proceedings of the 2020 ACM Symposium on Spatial User Interaction</em><b> (SUI ‚Äô20)</b>, Virtual Event, Canada, 2020 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/tango.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=RVXBBz1a8QU&amp;ab_channel=ACMSUI2020Conference" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We present TanGo, an always-available input modality on VR headset, which can be complementary to current VR accessories. TanGO is an active mechanical structure symmetrically equipped on Head-Mounted Display, enabling 3-dimensional bimanual sliding input with each degree of freedom furnished a brake system driven by micro servo generating totally 6 passive resistive force profiles. TanGo is an all-in-one structure that possess rich input and output while keeping compactness with the trade-offs between size, weight and usability. Users can actively gesture like pushing, shearing, or squeezing with specific output provided while allowing hands to rest in stationary experiences. TanGo also renders users flexibility to switch seamlessly between virtual and real usage in Augmented Reality without additional efforts and instruments. We demonstrate three applications to show the interaction space of TanGo and then discuss its limitation and show future possibilities based on preliminary user feedback.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/masque-480.webp 480w,/assets/img/publication_preview/masque-800.webp 800w,/assets/img/publication_preview/masque-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/masque.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="masque.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3332165.3347898" class="col-sm-8"> <div class="title">Masque: Exploring Lateral Skin Stretch Feedback on the Face with Head-Mounted Displays</div> <div class="author"> Chi Wang,¬†Da-Yuan Huang,¬†Shuo-wen Hsu,¬†Chu-En Hou,¬†Yeu-Luen Chiu,¬†<em>Ruei-Che Chang</em>,¬†Jo-Yu Lo,¬†and¬†Bing-Yu Chen </div> <div class="periodical"> <em>In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology</em><b> (UIST ‚Äô19)</b>, New Orleans, LA, USA, 2019 </div> <div class="periodical"> </div> <div class="award"></div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/masque.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=AuoENjRW5F0&amp;list=PLCm1azruPu2d703VUc7ouZ2JQSr58a25t&amp;index=6&amp;ab_channel=ACMSIGCHI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We propose integrating an array of skin stretch modules with an head-mounted display (HMD) to provide two-dimensional skin stretch feedback on the user‚Äôs face. Skin stretch has been found effective to induce the perception of force (e.g. weight or inertia) and to enable directional haptic cues. However, its potential as an HMD output for virtual reality (VR) remains to be exploited. Our explorative study firstly investigated the design of shear tactors. Based on our results, Masque has been implemented as an HMD prototype actuating six shear tactors positioned on the HMD‚Äôs face interface. A comfort study was conducted to ensure that skin stretches generated by Masque are acceptable to all participants. The following two perception-based studies examined the minimum changes in skin stretch distance and stretch angles that are detectable by participants. The results help us to design haptic profiles as well as our prototype applications. Finally, the user evaluation indicates that participants welcomed Masque and regarded skin stretch feedback as a worthwhile addition to HMD output.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Ruei-Che Chang. Last updated: April 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-3P6TGY38D9"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-3P6TGY38D9");</script> </body> </html>